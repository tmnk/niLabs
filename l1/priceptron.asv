X = [ 3 -3.8 -1.8 -1.1 -3.2 -4.8; ...
      2.4 0.2 0.4 -0.9 -2.5 4.2];
T = [0 1 1 1 1 0];
% hardlim Ступенчатая функция с жесткими ограничениями
% learnp Абсолютная функция настройки
net = perceptron('hardlim', 'learnp'); 
display(net);
net = configure(net, X, T);
W = rand(1,3) * 10 - 5
net.IW{1,1} = W(1:2);
net.b{1} = W(3);
plotpv(P,T), grid
PLTr = plotpc(net.IW{1,1}, net.b{1})
set(PLTr, 'LineStyle', '--');
net = init(net);
y = net(x);
mae(t - y)
net.trainParam.epochs = 50;
[net,tr] = train(net, X, T);
PLTr1 = plotpc(net.IW{1,1},net.b{1});
[~, k] = size(X);
finish = false;
for i = 1 : 50
    if finish == true
        break;
    end;
    finish = true;
    for j = 1 : k
        p = X(:,j);
        t = T(:,j);
        A = W*[p;1];
        A = A >= 0;
        e = t - A;
        if(~mae(e))
            continue;
        end;
        finish = false;
        W = W + e*([p; 1]'); 
    end;
end;

PLTr2 = plotpc(W(1:2),W(3));
set(PLTr2, 'LineStyle', '--');
set(PLTr2, 'Color', 'red');
legend([PLTr,PLTr1,PLTr2],'random', 'Net', 'MyPreceptron');
%%
X = [ 2 2.3 0.4 -1.9 -3.2 -0.4 4.1 -5; ...
      -1.3 4.5 0.4 -4.3 -4.1 -5 1.4 -4.7];
T = [ 0 0 0 1 1 1 0 1;...
      1 0 0 0 0 1 1 0];
net1 = perceptron('hardlim', 'learnp');
net2 = perceptron('hardlim', 'learnp');
net1 = configure(net1, X(1,:), T(1,:));
net2 = configure(net2, X(2,:), T(2,:));
W = rand(1,3) * 10 - 5
net1.IW{1,1} = W(1:2);
net1.b{1} = W(3);
net2.IW{1,1} = W(1:2);
net2.b{1} = W(3);
net1 = init(net1);
net2 = init(net2);
y = net1(x);
mae(t - y)
net1.trainParam.epochs = 50;
[net1,tr] = train(net1, X(1,:), T(1,:));
PLTr1 = plotpc(net1.IW{1,1},net1.b{1});